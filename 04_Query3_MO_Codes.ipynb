{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d73412-d536-45e1-9cd5-d1cd895b73ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c62a93f3564daa9284f0a2948fb4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created. App Name: Query3_Mocodes_Analysis"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, trim, regexp_replace, count, desc, broadcast\n",
    "\n",
    "# Configuration (4 Executors, 1 Core, 2GB RAM)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query3_Mocodes_Analysis\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "DATA_PATH = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(f\"Spark Session Created. App Name: {spark.conf.get('spark.app.name')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce8f89a-a5aa-4c72-8958-6c2de18b2362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c66e9399a6491c8a11e2816cd157d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared. Total MO Code Occurrences: 8,256,514"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "df_crime = spark.read.parquet(f\"{DATA_PATH}/crime_data_clean.parquet\")\n",
    "df_mo_codes = spark.read.parquet(f\"{DATA_PATH}/mo_codes.parquet\")\n",
    "\n",
    "# 2. Prepare MO Codes Mapping Table (Rename for clarity)\n",
    "df_mo_mapping = df_mo_codes \\\n",
    "    .withColumnRenamed(df_mo_codes.columns[0], \"code\") \\\n",
    "    .withColumnRenamed(df_mo_codes.columns[1], \"description\")\n",
    "\n",
    "# 3. Clean & Explode Crime MO Codes\n",
    "# Logic: Trim spaces -> Replace multiple spaces with single space -> Split -> Explode\n",
    "df_exploded = df_crime \\\n",
    "    .filter(col(\"Mocodes\").isNotNull()) \\\n",
    "    .select(\n",
    "        explode(\n",
    "            split(\n",
    "                regexp_replace(trim(col(\"Mocodes\")), r\"\\s+\", \" \"), \n",
    "                \" \"\n",
    "            )\n",
    "        ).alias(\"mo_code\")\n",
    "    )\n",
    "\n",
    "# Cache the exploded dataset as it is used by both implementations\n",
    "df_exploded.cache()\n",
    "print(f\"Data Prepared. Total MO Code Occurrences: {df_exploded.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12f383b-c366-411e-a594-7d2693662aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7541a0b56eb94c72a403c7f647d21350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame API: Join Strategy Comparison ---\n",
      "Strategy: Default         | Time: 5.1218s | Result Rows: 613\n",
      "Strategy: Broadcast       | Time: 2.5074s | Result Rows: 613\n",
      "Strategy: Merge           | Time: 5.3529s | Result Rows: 613\n",
      "--------------------------------------------------\n",
      "DataFrame API (Best Time): 2.5074s"
     ]
    }
   ],
   "source": [
    "print(\"--- DataFrame API: Join Strategy Comparison ---\")\n",
    "\n",
    "def benchmark_join_strategy(strategy_name, mapping_df):\n",
    "    spark.catalog.clearCache()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform Join\n",
    "    df_joined = df_exploded.join(\n",
    "        mapping_df,\n",
    "        df_exploded[\"mo_code\"] == mapping_df[\"code\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    \n",
    "    # Aggregation\n",
    "    count_result = df_joined.groupBy(\"code\", \"description\").count().count()\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Strategy: {strategy_name:<15} | Time: {duration:.4f}s | Result Rows: {count_result}\")\n",
    "    return duration\n",
    "\n",
    "# 1. Default Strategy (Let Optimizer decide)\n",
    "time_default = benchmark_join_strategy(\"Default\", df_mo_mapping)\n",
    "\n",
    "# 2. Broadcast Strategy (Force Broadcast)\n",
    "time_broadcast = benchmark_join_strategy(\"Broadcast\", broadcast(df_mo_mapping))\n",
    "\n",
    "# 3. Merge Strategy (Force SortMergeJoin)\n",
    "time_merge = benchmark_join_strategy(\"Merge\", df_mo_mapping.hint(\"merge\"))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"DataFrame API (Best Time): {min(time_default, time_broadcast, time_merge):.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a845a0-1ab0-41ba-86c0-4c63bba0f121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fc38360fdd4a6e95b9a89358c8cd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RDD API Implementation ---\n",
      "RDD API Time: 17.5667s\n",
      "\n",
      "Top 5 MO Codes (RDD Results):\n",
      "0344 - Removes vict property: 1002274\n",
      "1822 - Stranger: 547653\n",
      "0416 - Hit-Hit w/ weapon: 404276\n",
      "0329 - Vandalized: 377265\n",
      "0913 - Victim knew Suspect: 278086"
     ]
    }
   ],
   "source": [
    "print(\"--- RDD API Implementation ---\")\n",
    "spark.catalog.clearCache()\n",
    "start_time_rdd = time.time()\n",
    "\n",
    "# 1. Convert DataFrames to RDDs\n",
    "rdd_crimes = df_exploded.rdd.map(lambda row: (row[\"mo_code\"], 1))\n",
    "rdd_mapping = df_mo_mapping.rdd.map(lambda row: (row[\"code\"], row[\"description\"]))\n",
    "\n",
    "# 2. Join & Aggregate\n",
    "# Map -> ((Code, Desc), 1) -> ReduceByKey\n",
    "rdd_result = rdd_crimes \\\n",
    "    .join(rdd_mapping) \\\n",
    "    .map(lambda x: ((x[0], x[1][1]), x[1][0])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .collect()\n",
    "\n",
    "duration_rdd = time.time() - start_time_rdd\n",
    "print(f\"RDD API Time: {duration_rdd:.4f}s\")\n",
    "\n",
    "# Display Top 5 Results\n",
    "print(\"\\nTop 5 MO Codes (RDD Results):\")\n",
    "for item in rdd_result[:5]:\n",
    "    print(f\"{item[0][0]} - {item[0][1]}: {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f461474-ece2-470b-b38c-1bb4fa0c6d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
