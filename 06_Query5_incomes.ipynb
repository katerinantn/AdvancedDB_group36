{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2bb8e2-031b-4cfd-a7ac-7a4cdfb9af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1758</td><td>application_1765289937462_1742</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1742/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1742_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f1a1c2c68f47eeb95b50bb104e6525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d597319a01c546d88866f00558966880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o425.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 4 times, most recent failure: Lost task 3.3 in stage 8.0 (TID 17) (ip-192-168-1-156.eu-central-1.compute.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765289937462_1742_01_000008 on host: ip-192-168-1-156.eu-central-1.compute.internal. Exit status: 137. Diagnostics: [2025-12-15 15:40:22.951]Container killed on request. Exit code is 137\n",
      "[2025-12-15 15:40:22.951]Container exited with a non-zero exit code 137. \n",
      "[2025-12-15 15:40:22.951]Killed by external signal\n",
      ".\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4222)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4219)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1264, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o425.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 4 times, most recent failure: Lost task 3.3 in stage 8.0 (TID 17) (ip-192-168-1-156.eu-central-1.compute.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765289937462_1742_01_000008 on host: ip-192-168-1-156.eu-central-1.compute.internal. Exit status: 137. Diagnostics: [2025-12-15 15:40:22.951]Container killed on request. Exit code is 137\n",
      "[2025-12-15 15:40:22.951]Container exited with a non-zero exit code 137. \n",
      "[2025-12-15 15:40:22.951]Killed by external signal\n",
      ".\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4222)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4219)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "# Dynamic Allocation\n",
    "\n",
    "spark_conf = {\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "    \"spark.shuffle.service.enabled\": \"true\"\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query5_Clean_Run\") \\\n",
    "    .config(map=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(\"--- Loading Data ---\")\n",
    "\n",
    "\n",
    "blocks = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\").select(\n",
    "    F.col(\"features.properties.COMM\").getItem(0).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"total_pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "# CRIMES: Sampling 20% \n",
    "crimes = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\") \\\n",
    "    .filter((F.col(\"Year\") >= 2020)) \\\n",
    "    .sample(False, 0.2, 42) \\\n",
    "    .select(\"LON\", \"LAT\")\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# INCOME\n",
    "spark.read.parquet(f\"{base_path}/income_data.parquet\").select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "print(\"--- Executing Join ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    b.COMM,\n",
    "    AVG(i.income) as avg_income,\n",
    "    (COUNT(c.LON) * 5) / SUM(b.total_pop) as crime_rate -- x5 λόγω 20% sample\n",
    "FROM blocks b\n",
    "LEFT JOIN income i ON b.zcta = i.zip_code\n",
    "JOIN crimes c ON ST_Contains(b.geometry, ST_Point(CAST(c.LON AS Decimal(24,20)), CAST(c.LAT AS Decimal(24,20))))\n",
    "WHERE b.total_pop > 0 \n",
    "GROUP BY b.COMM\n",
    "HAVING avg_income IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Χρησιμοποιούμε limit για να δούμε αν ξεκινάει η ροή\n",
    "final_df = spark.sql(query)\n",
    "results = final_df.collect()\n",
    "\n",
    "print(f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "if results:\n",
    "    df = spark.createDataFrame(results)\n",
    "    print(f\"Correlation: {df.stat.corr('avg_income', 'crime_rate'):.5f}\")\n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"No results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bd26a-843e-4900-8eec-e971ea61cb84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
