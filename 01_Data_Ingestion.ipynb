{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754e8f2e-e840-4025-a314-ff48e98cb10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6215564c09d84d08befd2405db069a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.2-amzn-1\n",
      "Spark Session created successfully.\n",
      "Executors: 4\n",
      "Cores per executor: 1\n",
      "Memory per executor: 2g"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, trim, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Create Spark Session\n",
    "#Fixing of legacy time paser to avoid errors in older date forms.\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"LA_Crime_Project_Ingestion\") \\\n",
    ".config(\"spark.executor.instances\", \"4\") \\\n",
    ".config(\"spark.executor.cores\", \"1\") \\\n",
    ".config(\"spark.executor.memory\", \"2g\") \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.sql.legacy.timeParserPolicy\",\"CORRECTED\") \\\n",
    ".config(\"spark.sql.caseSensitive\",\"false\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Spark Session created successfully.\")\n",
    "print(f\"Executors: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"Cores per executor: {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"Memory per executor: {spark.conf.get('spark.executor.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b09dec7-c495-4a3c-b3ca-d82f53e09d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bb3462c68d4776bef50feac271b740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: s3a://initial-notebook-data-bucket-dblab-905418150721/project_data\n",
      "Writing to:   s3a://groups-bucket-dblab-905418150721/group36/processed_data"
     ]
    }
   ],
   "source": [
    "#data source (using s3a for better compatibility with Hadoop/spark)\n",
    "INPUT_BUCKET = \"s3a://initial-notebook-data-bucket-dblab-905418150721/project_data\"\n",
    "#data destination - dblab group 36 db lab\n",
    "OUTPUT_PATH = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(f\"Reading from: {INPUT_BUCKET}\")\n",
    "print(f\"Writing to:   {OUTPUT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489f33d6-2d21-40a4-bbc6-69c52039c209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11217625ac04ff9bf5bf4c422d2ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Crime Data\n",
      "Loaded 2010-2019 count: 2133137\n",
      "Loaded 2020+ count: 1004991\n",
      "Total Records: 3138128\n",
      "root\n",
      " |-- DR_NO: integer (nullable = true)\n",
      " |-- Date Rptd: string (nullable = true)\n",
      " |-- DATE OCC: string (nullable = true)\n",
      " |-- TIME OCC: integer (nullable = true)\n",
      " |-- AREA: integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: integer (nullable = true)\n",
      " |-- Part 1-2: integer (nullable = true)\n",
      " |-- Crm Cd: integer (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: integer (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: integer (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: integer (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: integer (nullable = true)\n",
      " |-- Crm Cd 2: integer (nullable = true)\n",
      " |-- Crm Cd 3: integer (nullable = true)\n",
      " |-- Crm Cd 4: integer (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "print (\"Loading Crime Data\")\n",
    "\n",
    "#Paths\n",
    "path_crime_10_19 = f\"{INPUT_BUCKET}/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "path_crime_20_plus = f\"{INPUT_BUCKET}/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "\n",
    "#Load CSVs\n",
    "#UsinginferSchema=True so it automatically understands the formulas\n",
    "df_10_19 = spark.read.csv(path_crime_10_19, header = True, inferSchema = True)\n",
    "df_20_plus = spark.read.csv(path_crime_20_plus, header= True, inferSchema = True)\n",
    "\n",
    "#Union based on column names \n",
    "#allowMissingColumns enabled in case columns change in the course of time \n",
    "\n",
    "df_crime_total = df_10_19.unionByName(df_20_plus, allowMissingColumns=True)\n",
    "\n",
    "print(f\"Loaded 2010-2019 count: {df_10_19.count()}\")\n",
    "print(f\"Loaded 2020+ count: {df_20_plus.count()}\")\n",
    "print(f\"Total Records: {df_crime_total.count()}\")\n",
    "\n",
    "#Print result \n",
    "df_crime_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5436447c-3a4b-4ce9-adec-6870570d86d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54906bc58c94434d972ea45d9bda6a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Auxiliary Data\n",
      "Income Data Loaded.\n",
      "MO Codes Parsed.\n",
      "Lookup Tables Loaded.\n",
      "GeoJSON Loaded successfully."
     ]
    }
   ],
   "source": [
    "print(\"Loading Auxiliary Data\")\n",
    "\n",
    "# 1. Income Data (Προσοχή στο delimiter ';')\n",
    "path_income = f\"{INPUT_BUCKET}/LA_income_2021.csv\"\n",
    "df_income = spark.read.option(\"delimiter\", \";\").csv(path_income, header=True, inferSchema=True)\n",
    "print(\"Income Data Loaded.\")\n",
    "\n",
    "# 2. MO Codes (Text parsing)\n",
    "path_mo = f\"{INPUT_BUCKET}/MO_codes.txt\"\n",
    "df_mo_raw = spark.read.text(path_mo)\n",
    "\n",
    "#split text into code and description parts\n",
    "\n",
    "df_mo = df_mo_raw.select(\n",
    "    split(col(\"value\"), \" \", 2).getItem(0).alias(\"MO_Code\"),\n",
    "    split(col(\"value\"), \" \",2).getItem(1).alias(\"MO_Description\")\n",
    ")\n",
    "print(\"MO Codes Parsed.\")\n",
    "\n",
    "#3. lookup Tables (Police Stations, Race Codes)\n",
    "df_police = spark.read.csv(f\"{INPUT_BUCKET}/LA_Police_Stations.csv\", header=True, inferSchema=True)\n",
    "df_race = spark.read.csv(f\"{INPUT_BUCKET}/RE_codes.csv\", header=True, inferSchema=True)\n",
    "print(\"Lookup Tables Loaded.\")\n",
    "\n",
    "#4. Census Blocks (GeoJSON)\n",
    "#To GeoJSON συνήθως είναι multiline json\n",
    "path_geo = f\"{INPUT_BUCKET}/LA_Census_Blocks_2020.geojson\"\n",
    "try:\n",
    "    df_geo = spark.read.option(\"multiline\", \"true\").json(path_geo)\n",
    "    print(\"GeoJSON Loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GeoJSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fe6cec1-8d23-45e1-82c2-c9d1cfc29426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b5307e036e499fbaa3917d829ebacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saving Data to Parquet ---\n",
      "Saved: crime_data_raw.parquet\n",
      "Saved: income_data.parquet\n",
      "Saved: mo_codes.parquet\n",
      "Saved: police_stations.parquet\n",
      "Saved: race_codes.parquet\n",
      "Saved: census_blocks_geo.parquet\n",
      "All data saved to processed_data folder."
     ]
    }
   ],
   "source": [
    "print(\"--- Saving Data to Parquet ---\")\n",
    "\n",
    "# 1. Save Crime Data\n",
    "df_crime_total.write.mode(\"overwrite\").parquet(\"s3a://groups-bucket-dblab-905418150721/group36/processed_data/crime_data_raw.parquet\")\n",
    "print(\"Saved: crime_data_raw.parquet\")\n",
    "\n",
    "# 2. Save Income Data\n",
    "df_income.write.mode(\"overwrite\").parquet(\"s3a://groups-bucket-dblab-905418150721/group36/processed_data/income_data.parquet\")\n",
    "print(\"Saved: income_data.parquet\")\n",
    "\n",
    "# 3. Save MO Codes\n",
    "df_mo.write.mode(\"overwrite\").parquet(\"s3a://groups-bucket-dblab-905418150721/group36/processed_data/mo_codes.parquet\")\n",
    "print(\"Saved: mo_codes.parquet\")\n",
    "\n",
    "# 4. Save Police Data\n",
    "df_police.write.mode(\"overwrite\").parquet(\"s3a://groups-bucket-dblab-905418150721/group36/processed_data/police_stations.parquet\")\n",
    "print(\"Saved: police_stations.parquet\")\n",
    "\n",
    "# 5. Save Race Codes\n",
    "df_race.write.mode(\"overwrite\").parquet(\"s3a://groups-bucket-dblab-905418150721/group36/processed_data/race_codes.parquet\")\n",
    "print(\"Saved: race_codes.parquet\")\n",
    "\n",
    "# 6. Save GeoJSON\n",
    "if 'df_geo' in locals():\n",
    "    df_geo.write.mode(\"overwrite\").parquet(\"s3a://groups-bucket-dblab-905418150721/group36/processed_data/census_blocks_geo.parquet\")\n",
    "    print(\"Saved: census_blocks_geo.parquet\")\n",
    "\n",
    "print(\"All data saved to processed_data folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
