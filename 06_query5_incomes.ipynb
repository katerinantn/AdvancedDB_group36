{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499f364-fcde-464d-9ce8-a5072c0ba5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# ==========================================\n",
    "# FULL RUN CONFIGURATION (Optimized)\n",
    "# ==========================================\n",
    "spark_conf = {\n",
    "    # Προσπαθούμε να πάρουμε όση μνήμη μας επιτρέπει το σύστημα\n",
    "    \"spark.executor.memory\": \"6g\", \n",
    "    \"spark.driver.memory\": \"6g\",\n",
    "    \"spark.executor.cores\": \"2\",\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \n",
    "    # Sedona & Kryo (Απαραίτητα για ταχύτητα)\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \n",
    "    # Αυξάνουμε το όριο για το αυτόματο Broadcast (για σιγουριά)\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\": \"204857600\" # 200MB\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query5_Full_Broadcast\") \\\n",
    "    .config(map=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(\"--- 1. Loading & Optimizing Census Blocks ---\")\n",
    "blocks_raw = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\")\n",
    "\n",
    "# Διόρθωση Array Types (.getItem(0)) + Επιλογή μόνο των απαραίτητων πεδίων\n",
    "blocks = blocks_raw.select(\n",
    "    F.upper(F.trim(F.col(\"features.properties.COMM\").getItem(0))).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"total_pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "\n",
    "# ΚΡΙΣΙΜΟ: Κάνουμε Cache τα Blocks. \n",
    "# Για να δουλέψει καλά το Broadcast, το Spark πρέπει να ξέρει ότι είναι μικρός πίνακας.\n",
    "blocks.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "n_blocks = blocks.count()\n",
    "print(f\"Blocks Loaded & Cached: {n_blocks}\")\n",
    "\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "print(\"--- 2. Loading Crimes (Full Dataset) ---\")\n",
    "crimes_raw = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\")\n",
    "\n",
    "# Φιλτράρουμε μόνο τα έτη που μας νοιάζουν και κρατάμε ΜΟΝΟ συντεταγμένες\n",
    "# για να ελαφρύνουμε το DataFrame όσο γίνεται.\n",
    "crimes = crimes_raw.filter((F.col(\"Year\") == 2020) | (F.col(\"Year\") == 2021)) \\\n",
    "                   .select(\n",
    "                       F.col(\"LON\").cast(\"decimal(24,20)\"), \n",
    "                       F.col(\"LAT\").cast(\"decimal(24,20)\")\n",
    "                   )\n",
    "\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "print(\"Crimes view created (Lazy Load).\")\n",
    "\n",
    "print(\"--- 3. Loading Income ---\")\n",
    "income = spark.read.parquet(f\"{base_path}/income_data.parquet\")\n",
    "income.select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"median_income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION (The Moment of Truth)\n",
    "# ==========================================\n",
    "print(\"--- 4. Executing Spatial Join with BROADCAST ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Χρησιμοποιούμε το /*+ BROADCAST(b) */ για να είμαστε σίγουροι\n",
    "query = \"\"\"\n",
    "WITH CrimeCounts AS (\n",
    "    SELECT /*+ BROADCAST(b) */\n",
    "        b.COMM,\n",
    "        COUNT(*) as total_crimes\n",
    "    FROM crimes c, blocks b\n",
    "    WHERE ST_Contains(b.geometry, ST_Point(c.LON, c.LAT))\n",
    "    GROUP BY b.COMM\n",
    "),\n",
    "AreaStats AS (\n",
    "    SELECT \n",
    "        b.COMM,\n",
    "        SUM(b.total_pop) as population,\n",
    "        AVG(i.median_income) as avg_income\n",
    "    FROM blocks b\n",
    "    LEFT JOIN income i ON b.zcta = i.zip_code\n",
    "    GROUP BY b.COMM\n",
    ")\n",
    "SELECT \n",
    "    a.COMM,\n",
    "    a.avg_income,\n",
    "    a.population,\n",
    "    (c.total_crimes / 2.0) / (a.population / 1000.0) as crime_rate_per_1k\n",
    "FROM AreaStats a\n",
    "JOIN CrimeCounts c ON a.COMM = c.COMM\n",
    "WHERE a.population > 0 AND a.avg_income IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Εκτέλεση\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "print(\"Calculating results (this might take a minute)...\")\n",
    "results = result_df.collect()\n",
    "\n",
    "print(f\"Processing finished. Found {len(results)} communities.\")\n",
    "\n",
    "if len(results) > 0:\n",
    "    res_df = spark.createDataFrame(results)\n",
    "    \n",
    "    # Υπολογισμός Correlation\n",
    "    corr = res_df.stat.corr(\"avg_income\", \"crime_rate_per_1k\")\n",
    "    \n",
    "    # Ταξινόμηση τοπικά (στον Driver) για να μην κουράσουμε το Spark\n",
    "    top10_high = res_df.orderBy(F.col(\"avg_income\").desc()).limit(10)\n",
    "    top10_low = res_df.orderBy(F.col(\"avg_income\").asc()).limit(10)\n",
    "    \n",
    "    try:\n",
    "        c_high = top10_high.stat.corr(\"avg_income\", \"crime_rate_per_1k\")\n",
    "        c_low = top10_low.stat.corr(\"avg_income\", \"crime_rate_per_1k\")\n",
    "    except:\n",
    "        c_high, c_low = 0.0, 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"RESULTS (FULL DATASET)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Global Correlation:          {corr:.5f}\")\n",
    "    print(f\"Correlation (Richest 10):    {c_high:.5f}\")\n",
    "    print(f\"Correlation (Poorest 10):    {c_low:.5f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total Time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    res_df.select(\"COMM\", \"avg_income\", \"crime_rate_per_1k\").show(5)\n",
    "else:\n",
    "    print(\"No data returned.\")\n",
    "\n",
    "# Cleanup\n",
    "blocks.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d618ac-fc9f-4e67-ac88-14006877e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "# Ρυθμίσεις για να τρέξει σίγουρα\n",
    "spark_conf = {\n",
    "    \"spark.executor.instances\": \"1\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.executor.memory\": \"4g\",\n",
    "    \"spark.sql.shuffle.partitions\": \"200\", \n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\"\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Query5_Final_Result\").config(map=spark_conf).getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "# Blocks (με Array Fix)\n",
    "blocks = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\").select(\n",
    "    F.col(\"features.properties.COMM\").getItem(0).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "# Crimes (10% Sample)\n",
    "crimes = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\") \\\n",
    "    .filter((F.col(\"Year\") >= 2020)) \\\n",
    "    .sample(False, 0.10, 42) \\\n",
    "    .select(\"LON\", \"LAT\")\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# Income\n",
    "spark.read.parquet(f\"{base_path}/income_data.parquet\").select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "# Query\n",
    "start = time.time()\n",
    "res = spark.sql(\"\"\"\n",
    "WITH CrimeCounts AS (\n",
    "    SELECT /*+ BROADCAST(b) */ b.COMM, COUNT(*) * 10 as crimes FROM crimes c, blocks b\n",
    "    WHERE ST_Contains(b.geometry, ST_Point(CAST(c.LON AS Decimal(24,20)), CAST(c.LAT AS Decimal(24,20))))\n",
    "    GROUP BY b.COMM\n",
    "),\n",
    "Stats AS (\n",
    "    SELECT b.COMM, SUM(b.pop) as pop, AVG(i.income) as income FROM blocks b\n",
    "    LEFT JOIN income i ON b.zcta = i.zip_code GROUP BY b.COMM\n",
    ")\n",
    "SELECT s.COMM, s.income, (c.crimes/s.pop) as rate \n",
    "FROM Stats s JOIN CrimeCounts c ON s.COMM = c.COMM \n",
    "WHERE s.pop > 500 AND s.income IS NOT NULL\n",
    "\"\"\").collect()\n",
    "\n",
    "if res:\n",
    "    df = spark.createDataFrame(res)\n",
    "    print(f\"Global Correlation: {df.stat.corr('income', 'rate'):.5f}\")\n",
    "    print(f\"Time: {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd441f5-3096-4cb4-b356-34df9262e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "# Ελάχιστες ρυθμίσεις για να μην πιάσει πόρους\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query5_ProofOfConcept\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(\"--- Loading Data (Tiny Sample) ---\")\n",
    "\n",
    "# Blocks\n",
    "blocks = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\").select(\n",
    "    F.col(\"features.properties.COMM\").getItem(0).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "# Crimes -> 0.1% SAMPLE (Πολύ μικρό για να τρέξει αμέσως)\n",
    "crimes = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\") \\\n",
    "    .filter((F.col(\"Year\") >= 2020)) \\\n",
    "    .sample(False, 0.001, 42) \\\n",
    "    .select(\"LON\", \"LAT\")\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# Income\n",
    "spark.read.parquet(f\"{base_path}/income_data.parquet\").select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "print(\"--- Executing Logic ---\")\n",
    "start = time.time()\n",
    "\n",
    "# Το Query\n",
    "res = spark.sql(\"\"\"\n",
    "WITH CrimeCounts AS (\n",
    "    SELECT /*+ BROADCAST(b) */ b.COMM, COUNT(*) as crimes \n",
    "    FROM crimes c, blocks b\n",
    "    WHERE ST_Contains(b.geometry, ST_Point(CAST(c.LON AS Decimal(24,20)), CAST(c.LAT AS Decimal(24,20))))\n",
    "    GROUP BY b.COMM\n",
    "),\n",
    "Stats AS (\n",
    "    SELECT b.COMM, SUM(b.pop) as pop, AVG(i.income) as income FROM blocks b\n",
    "    LEFT JOIN income i ON b.zcta = i.zip_code GROUP BY b.COMM\n",
    ")\n",
    "SELECT s.COMM, s.income, (c.crimes) as total_hits \n",
    "FROM Stats s JOIN CrimeCounts c ON s.COMM = c.COMM \n",
    "WHERE s.pop > 0 AND s.income IS NOT NULL\n",
    "\"\"\").limit(100).collect() # Limit για να τελειώνει\n",
    "\n",
    "print(f\"Time: {time.time()-start:.2f}s\")\n",
    "\n",
    "if res:\n",
    "    df = spark.createDataFrame(res)\n",
    "    # Απλά για να δούμε αν βγάζει κάτι\n",
    "    df.show(5)\n",
    "    print(\"Code executed successfully on sample.\")\n",
    "else:\n",
    "    print(\"No intersection found in this tiny sample (Expected).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97b94d-b945-42ca-94f8-3177f34d6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "# Βασικές ρυθμίσεις\n",
    "spark_conf = {\n",
    "    \"spark.executor.instances\": \"1\",\n",
    "    \"spark.executor.cores\": \"2\",\n",
    "    \"spark.executor.memory\": \"4g\",\n",
    "    \"spark.sql.shuffle.partitions\": \"200\", \n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\"\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Query5_Smart_Grid\").config(map=spark_conf).getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(\"--- 1. Loading Blocks ---\")\n",
    "blocks = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\").select(\n",
    "    F.col(\"features.properties.COMM\").getItem(0).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"total_pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "print(\"--- 2. Smart Aggregation of Crimes ---\")\n",
    "# ΑΝΤΙ ΝΑ ΦΟΡΤΩΣΟΥΜΕ ΟΛΑ ΤΑ ΕΓΚΛΗΜΑΤΑ ΓΙΑ JOIN:\n",
    "# 1. Φιλτράρουμε έτη\n",
    "# 2. Στρογγυλοποιούμε (Round) τις συντεταγμένες σε 3 δεκαδικά (~110 μέτρα ακρίβεια)\n",
    "# 3. Κάνουμε Group By και Count ΠΡΙΝ το Join\n",
    "crimes_raw = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\")\n",
    "\n",
    "crime_grid = crimes_raw.filter((F.col(\"Year\") >= 2020)) \\\n",
    "    .withColumn(\"lat_grid\", F.round(F.col(\"LAT\"), 3)) \\\n",
    "    .withColumn(\"lon_grid\", F.round(F.col(\"LON\"), 3)) \\\n",
    "    .groupBy(\"lat_grid\", \"lon_grid\") \\\n",
    "    .agg(F.count(\"*\").alias(\"crime_count\"))\n",
    "\n",
    "# Αυτό το DataFrame είναι ΠΟΛΥ μικρότερο από το αρχικό\n",
    "crime_grid.createOrReplaceTempView(\"crime_grid\")\n",
    "\n",
    "count_grid = crime_grid.count()\n",
    "print(f\"Compressed 2M+ crimes into {count_grid} grid points.\")\n",
    "\n",
    "print(\"--- 3. Loading Income ---\")\n",
    "spark.read.parquet(f\"{base_path}/income_data.parquet\").select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"median_income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "print(\"--- 4. Executing Optimized Spatial Join ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Τώρα κάνουμε Join τα Grid Points (λίγα) με τα Blocks\n",
    "# Είναι πολύ πιο γρήγορο γιατί έχουμε λιγότερα σημεία να ελέγξουμε.\n",
    "metrics_query = \"\"\"\n",
    "WITH LocalCrime AS (\n",
    "    SELECT /*+ BROADCAST(b) */ \n",
    "        b.COMM,\n",
    "        SUM(g.crime_count) as total_crimes -- Αθροίζουμε τα counts των grid points\n",
    "    FROM crime_grid g, blocks b\n",
    "    WHERE ST_Contains(b.geometry, ST_Point(CAST(g.lon_grid AS Decimal(24,20)), CAST(g.lat_grid AS Decimal(24,20))))\n",
    "    GROUP BY b.COMM\n",
    "),\n",
    "EnrichedBlocks AS (\n",
    "    SELECT \n",
    "        b.COMM,\n",
    "        SUM(b.total_pop) as population,\n",
    "        AVG(i.median_income) as avg_income\n",
    "    FROM blocks b\n",
    "    LEFT JOIN income i ON b.zcta = i.zip_code\n",
    "    GROUP BY b.COMM\n",
    ")\n",
    "SELECT \n",
    "    e.COMM,\n",
    "    e.avg_income,\n",
    "    e.population,\n",
    "    (COALESCE(c.total_crimes, 0) / 2.0) / NULLIF(e.population, 0) as crime_rate\n",
    "FROM EnrichedBlocks e\n",
    "LEFT JOIN LocalCrime c ON e.COMM = c.COMM\n",
    "WHERE e.population > 0 AND e.avg_income IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "final_df = spark.sql(metrics_query)\n",
    "results = final_df.collect()\n",
    "\n",
    "if len(results) > 0:\n",
    "    res_df = spark.createDataFrame(results)\n",
    "    \n",
    "    corr_all = res_df.stat.corr(\"avg_income\", \"crime_rate\")\n",
    "    \n",
    "    # Sort locally\n",
    "    top10_high = res_df.orderBy(F.col(\"avg_income\").desc()).limit(10)\n",
    "    top10_low = res_df.orderBy(F.col(\"avg_income\").asc()).limit(10)\n",
    "    \n",
    "    try:\n",
    "        corr_high = top10_high.stat.corr(\"avg_income\", \"crime_rate\")\n",
    "        corr_low = top10_low.stat.corr(\"avg_income\", \"crime_rate\")\n",
    "    except:\n",
    "        corr_high, corr_low = 0.0, 0.0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"RESULTS (Smart Grid Aggregation)\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Correlation (Global):          {corr_all:.5f}\")\n",
    "    print(f\"Correlation (Top 10 Richest):  {corr_high:.5f}\")\n",
    "    print(f\"Correlation (Top 10 Poorest):  {corr_low:.5f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Time: {time.time() - start_time:.2f} s\")\n",
    "    \n",
    "    res_df.select(\"COMM\", \"avg_income\", \"crime_rate\").show(5)\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63420ba9-f2b3-4d67-9933-85243d77ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "# Ρυθμίσεις \"Safe Mode\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query5_Final_Sampling\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(\"--- 1. Loading Blocks (Array Fix) ---\")\n",
    "blocks = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\").select(\n",
    "    F.col(\"features.properties.COMM\").getItem(0).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "print(\"--- 2. Loading Crimes (10% SAMPLE) ---\")\n",
    "# ΑΥΤΟ ΕΙΝΑΙ ΤΟ ΚΛΕΙΔΙ: Παίρνουμε το 10%\n",
    "crimes = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\") \\\n",
    "    .filter((F.col(\"Year\") >= 2020)) \\\n",
    "    .sample(withReplacement=False, fraction=0.10, seed=42) \\\n",
    "    .select(\"LON\", \"LAT\")\n",
    "\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "print(f\"Sample size: {crimes.count()} records\")\n",
    "\n",
    "print(\"--- 3. Loading Income ---\")\n",
    "spark.read.parquet(f\"{base_path}/income_data.parquet\").select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "print(\"--- 4. Executing Spatial Join ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Το Query\n",
    "query = \"\"\"\n",
    "WITH CrimeCounts AS (\n",
    "    SELECT /*+ BROADCAST(b) */ \n",
    "        b.COMM,\n",
    "        COUNT(*) * 10 as est_crimes -- Scaling back up (x10)\n",
    "    FROM crimes c, blocks b\n",
    "    WHERE ST_Contains(b.geometry, ST_Point(CAST(c.LON AS Decimal(24,20)), CAST(c.LAT AS Decimal(24,20))))\n",
    "    GROUP BY b.COMM\n",
    "),\n",
    "AreaStats AS (\n",
    "    SELECT \n",
    "        b.COMM,\n",
    "        SUM(b.pop) as total_pop,\n",
    "        AVG(i.income) as avg_income\n",
    "    FROM blocks b\n",
    "    LEFT JOIN income i ON b.zcta = i.zip_code\n",
    "    GROUP BY b.COMM\n",
    ")\n",
    "SELECT \n",
    "    a.COMM,\n",
    "    a.avg_income,\n",
    "    (c.est_crimes / a.total_pop) as crime_rate\n",
    "FROM AreaStats a\n",
    "JOIN CrimeCounts c ON a.COMM = c.COMM\n",
    "WHERE a.total_pop > 500 AND a.avg_income IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "results = result_df.collect()\n",
    "\n",
    "if len(results) > 0:\n",
    "    df_res = spark.createDataFrame(results)\n",
    "    corr = df_res.stat.corr(\"avg_income\", \"crime_rate\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"RESULTS (10% Sample)\")\n",
    "    print(f\"Correlation: {corr:.5f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Time: {time.time() - start_time:.2f} s\")\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2bb8e2-031b-4cfd-a7ac-7a4cdfb9af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1758</td><td>application_1765289937462_1742</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1742/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1742_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f1a1c2c68f47eeb95b50bb104e6525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d597319a01c546d88866f00558966880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o425.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 4 times, most recent failure: Lost task 3.3 in stage 8.0 (TID 17) (ip-192-168-1-156.eu-central-1.compute.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765289937462_1742_01_000008 on host: ip-192-168-1-156.eu-central-1.compute.internal. Exit status: 137. Diagnostics: [2025-12-15 15:40:22.951]Container killed on request. Exit code is 137\n",
      "[2025-12-15 15:40:22.951]Container exited with a non-zero exit code 137. \n",
      "[2025-12-15 15:40:22.951]Killed by external signal\n",
      ".\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4222)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4219)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1264, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1765289937462_1742/container_1765289937462_1742_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o425.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 4 times, most recent failure: Lost task 3.3 in stage 8.0 (TID 17) (ip-192-168-1-156.eu-central-1.compute.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1765289937462_1742_01_000008 on host: ip-192-168-1-156.eu-central-1.compute.internal. Exit status: 137. Diagnostics: [2025-12-15 15:40:22.951]Container killed on request. Exit code is 137\n",
      "[2025-12-15 15:40:22.951]Container exited with a non-zero exit code 137. \n",
      "[2025-12-15 15:40:22.951]Killed by external signal\n",
      ".\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4222)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4219)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "# 1. Καθαρή Configuration (Dynamic Allocation)\n",
    "# Αφήνουμε το Spark να διαχειριστεί τη μνήμη όπως ξέρει\n",
    "spark_conf = {\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.kryo.registrator\": \"org.apache.sedona.core.serde.SedonaKryoRegistrator\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"true\", # <--- Το κλειδί αν δουλεύει σε άλλους\n",
    "    \"spark.shuffle.service.enabled\": \"true\"\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query5_Clean_Run\") \\\n",
    "    .config(map=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "base_path = \"s3a://groups-bucket-dblab-905418150721/group36/processed_data\"\n",
    "\n",
    "print(\"--- Loading Data ---\")\n",
    "\n",
    "# BLOCKS: Διόρθωση Array μόνο\n",
    "blocks = spark.read.parquet(f\"{base_path}/census_blocks_geo.parquet\").select(\n",
    "    F.col(\"features.properties.COMM\").getItem(0).alias(\"COMM\"),\n",
    "    F.col(\"features.properties.POP20\").getItem(0).cast(\"long\").alias(\"total_pop\"),\n",
    "    F.col(\"features.properties.ZCTA20\").getItem(0).cast(\"long\").cast(\"string\").alias(\"zcta\"),\n",
    "    F.expr(\"ST_GeomFromGeoJSON(to_json(features.geometry))\").alias(\"geometry\")\n",
    ")\n",
    "blocks.createOrReplaceTempView(\"blocks\")\n",
    "\n",
    "# CRIMES: Sampling 20% (Αρκετό για να τρέξει, αρκετό για αποτελέσματα)\n",
    "crimes = spark.read.parquet(f\"{base_path}/crime_data_clean.parquet\") \\\n",
    "    .filter((F.col(\"Year\") >= 2020)) \\\n",
    "    .sample(False, 0.2, 42) \\\n",
    "    .select(\"LON\", \"LAT\")\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# INCOME\n",
    "spark.read.parquet(f\"{base_path}/income_data.parquet\").select(\n",
    "    F.col(\"Zip Code\").cast(\"long\").cast(\"string\").alias(\"zip_code\"),\n",
    "    F.col(\"Estimated Median Income\").alias(\"income\")\n",
    ").createOrReplaceTempView(\"income\")\n",
    "\n",
    "print(\"--- Executing Join ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    b.COMM,\n",
    "    AVG(i.income) as avg_income,\n",
    "    (COUNT(c.LON) * 5) / SUM(b.total_pop) as crime_rate -- x5 λόγω 20% sample\n",
    "FROM blocks b\n",
    "LEFT JOIN income i ON b.zcta = i.zip_code\n",
    "JOIN crimes c ON ST_Contains(b.geometry, ST_Point(CAST(c.LON AS Decimal(24,20)), CAST(c.LAT AS Decimal(24,20))))\n",
    "WHERE b.total_pop > 0 \n",
    "GROUP BY b.COMM\n",
    "HAVING avg_income IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Χρησιμοποιούμε limit για να δούμε αν ξεκινάει η ροή\n",
    "final_df = spark.sql(query)\n",
    "results = final_df.collect()\n",
    "\n",
    "print(f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "if results:\n",
    "    df = spark.createDataFrame(results)\n",
    "    print(f\"Correlation: {df.stat.corr('avg_income', 'crime_rate'):.5f}\")\n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"No results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bd26a-843e-4900-8eec-e971ea61cb84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
